---
title: "Notes on Confidence Intervals"
author: "CSC 400, Spring 2024"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
runtime: shiny
resource_files:
- computation/app.R
- mean/app.R
- sampling/app.R
- proportion/app.R
---

```{r setup, include=FALSE}
## this doucment is published to:
## https://homer.shinyapps.io/stoveteam/
library(tidyverse)
library(readxl)
library(shiny)
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

## Introducton

Coming soon!




## Point Estimation and Sampling Variability

Coming soon!

<!-- Eivory does this app-->

```{r sampling, echo=FALSE}
shinyAppDir(
  appDir = "sampling",
  options = list(
    width = "100%", height = 550
  )
)
```


## Confidence Intervals for a Proportion

Coming soon!

<!-- Finn does this app-->

```{r proportion, echo=FALSE}
shinyAppDir(
  appDir = "proportion",
  options = list(
    width = "100%", height = 550
  )
)
```


## Confidence Intervals for the Mean

Coming soon!
 
<!-- Rhys does this app-->


```{r mean, echo=FALSE}
shinyAppDir(
  appDir = "mean",
  options = list(
    width = "100%", height = 550
  )
)
```


## Computation From the Data

### A Parametric Approach

We imagine a large population of households.  Each household $h$ in the population has a mean per-capta daily usage of wood, which we will denote $\mu_h$.  Of course $\mu_h$ varies from one household to another.  Let $\mu$ denote the mean of all these mean per-capita daily usages.  $\mu$ is the number that we are trying to estimate and for which we desire to form a confidence interval, based upon the data we have collected.

We make the assumption that the population of $\mu_h$-values is normallly distributed, with mean $\mu$ and some unknown standard deviation $\sigma_b$.  (The subscript $b$ calls to mind that the mean daily per-capita usage varies *between* housefolds, i.e., from one household to another.)

We select $n$ households at random from our population, and visit each household.  Let $i$ be any number from 1 to $n$, and consider household $i$ (the $i$-th household selected for study).  When we arrive at household $i$, we are of course not provided its $\mu_i$ value.  Instead we must estimate $\mu_i$ by measuring the per-capita usage for each of $J_i$ days (in the study conducted, $J_i$ is sometimes 3 and sometimes 4).  We get $J_i$ measurements:

$$X_{i1}, X_{i2}, \ldots, X_{iJ_i}.$$
We compute the mean of these measurements:

$$\bar{X}_i = \frac{\sum_{j=1}^{J_i}X_{ij}}{J_i}.$$
to estimate $\mu_i$.

Now there is variability in this estimator $\bar{X}_i$, because the actual per-capita usage in a single household varies from day to day.  We make an important simplifying assumption, namely:  that the one-day per-capita usage for the household is normally distributed, with mean $\mu_i$ and some unknown standard deviation $\sigma_w$. (The subscript $w$ in the notation calls to mind that the variability is "within" a fixed household from day to day.) Importantly, we are assuming that $\sigma_w$ is the *same* for all households in the population.

We would then estimate $\mu$ by computing the mean of the $n$ household means:

$$\overline{\overline{X}} = \frac{\sum_{i=1}^n \bar{X}_i}{n}$$

Note that there are *two* sources of variability in the above estimator:

* variability in mean daily per-capita usage from one household to another, expressed by the unknown $\sigma_b$;
* variability in per-capita usage from day to day, *within* a fixed household, expressed by the unkown $\sigma_w$.

In fact, probability theory tells us that the distribution of $\overline{\overline{X}}$ is normal, with mean $\mu$ and standard deviation:

$$\frac{\sqrt{\sigma^2_b +\frac{\sigma^2_w\left(1/J_1+\ldots+1/J_n\right)}{n}}}{\sqrt{n}}.$$

Now let us turn to the task of making confidence intervals for $\mu$.  To this end, define the *between-sum of squares*:

$$SS_b = \sum_{i=1}^n \left( \overline{\overline{X}} - \bar{X}_i \right)^2.$$

A bit of standard statistical theory tells us that if all households were measured for the same number $J$ of days, then the random variable:

$$T = \frac{\overline{\overline{X}} - \mu}{\sqrt{\frac{SS_b}{n(n-1)}}}$$
would have a t-distribution, with degrees of freedom equal to $n-1$.  (Interestingly, the degrees of freedom are independent of $J$.)

One then derives, in a way familiar to statisticians, confidence intervals of the form:

$$\left(\overline{\overline{X}} - t_{n-1}^*\sqrt{\frac{SS_b}{n(n-1)}},\, \overline{\overline{X}} + t_{n-1}^*\sqrt{\frac{SS_b}{n(n-1)}}\right),$$

where $t_{n-1}^*$ is the quantile of the t-distribution with $n-1$ degrees of freedom that is appropriate for the desired level of confidence.

Clearly, though,  there is an obstacle to using the above interval, since in our study not all households were measured for the same number of days:  the number of days was sometimes 3 and sometimes 4.  Hence the random variable $T$ will not, for us, have an exact t-distribution.  We won't be able to derive confidence intervals with coverage-properties that are known exactly.

We can take a conservative approach, however.  Suppose that we were to simply throw away the final measurement for any day that had four measurements.  Then the number $J$ above could be set to three, and from the remaining data we could compute exact confidence intervals.

```{r}
summary_data <- read_excel(
  "st_files/For Review_Ret_Justa_July KPT Complete_KPT_4day (1).xlsx", 
  range = "J22:Q38"
)

names(summary_data) <-
  c(
    "per_cap_mean",
    "weighted_per_cap_mean",
    "sd",
    "var",
    "weighted_var",
    "days_measured",
    "df",
    "cov"
  )


find_replicates <- function(fn, n) {
  replicates <- vector(mode = "list", length = n)
  for (i in 1:length(replicates)) {
    vals <- read_excel(
      path = fn,
      range = "I15:L15",
      sheet = paste0("HH", i, " Data"),
      col_names = FALSE
    ) %>% 
      as.matrix() %>% 
      t() %>% 
      .[,1]
    replicates[[i]] <- vals[!is.na(vals)]
  }
  replicates
}

measures <- find_replicates(
  fn = "st_files/For Review_Ret_Justa_July KPT Complete_KPT_4day (1).xlsx",
  n = nrow(summary_data)
)

compute_ci_1 <- function(data, level, use = NULL, r) {
  n <- nrow(data)
  sample_mean <- sum(data$per_cap_mean) / n
  multiplier <- qnorm((1 + level) / 2)
  if (is.null(use)) {
    ss_between <- sum((sample_mean - data$per_cap_mean)^2)
  } else {
    avgs <- numeric(length(r))
    for (i in 1:length(r)) {
      avgs[i] <- mean(r[[i]][1:use])
    }
    ss_between <- sum((sample_mean - avgs)^2)
  }
  margin <- multiplier * sqrt(ss_between / (n * (n - 1)))
  list(
    point_estimate = sample_mean, 
    margin = margin,
    interval = c(
      sample_mean - margin,
      sample_mean + margin
    )
  )
}

## try it out:
res <- compute_ci_1(
  data = summary_data, 
  level = 0.90,
  use = 3,
  r = measures
)
```


Accordingly, we throw out the fourth-day measurements and make a confidence interval based on the above formula, where each household has three measurements.

We find:

* The point-estimate of $\mu$ is `r round(res$point_estimate, 3)`.
* The 90%-confidence interval extends from `r round(res$interval[1], 3)` to `r round(res$interval[2], 3)`.
* The margin of error for the above interval is `r round(res$margin, 3)`.


### A Caution

Recall that one of our assumptions was that the underlying population of mean per-capita household usages was normally distributed.  In practice, no population distribution is ever exactly normal.  We can take some comfort from statistical theory, which assures us that even if the population is not normal, the actual coverage-properties of our confidence intervals converge to the advertised coverage-properties as sample size increases to infinity. Nonetheless, when the sample size is small and the population distribution differs significantly from normal (especially if it is skewed in one direction or another) then there may be a significant difference between actual and advertised performance.

Our sample size is only 16 households--somewhat small--so we should plot the sample to see whether there any indication of skewness in the population.  Below is a density plot of the sample:

```{r}
ggplot(summary_data, aes(x = per_cap_mean)) +
  geom_density(fill = "skyblue") +
  geom_rug()
```

The sample has a strong right-skew, indicating that the population could be strongly right-skewed as well.

In future studies it would be prudent to take a larger sample of households, perhaps 30 or so.

### The Bootstrap

Note:  More on the bootstrap in subsequent drafts.  Note that we get an interval rather similar to the one obtained above.


```{r}
lst <- vector(mode = "list", length = nrow(summary_data))
for (i in 1:length(lst)) {
  vals <- read_excel(
    "st_files/For Review_Ret_Justa_July KPT Complete_KPT_4day (1).xlsx", 
    range = "I15:L15",
    sheet = paste0("HH", i, " Data"),
    col_names = FALSE
  ) %>% 
    as.matrix() %>% 
    t() %>% 
    .[,1]
  vals <- vals[!is.na(vals)]
  lst[[i]] <- vals
}

set.seed(2424)
resampled_means <- function(data) {
  n <- length(data)
  means <- numeric(n)
  for (i in 1:n) {
    grp <- data[[sample(1:n, size = 1)]]
    means[i] <-
      sample(grp, size = length(grp), replace = TRUE) %>% 
      mean()
  }
  means
}

## try it:

bootstrap_resamples <- function(m, data) {
  resamps <- numeric(m)
  for (i in 1:m) {
    resamps[i] <- mean(resampled_means(data = data))
  }
  resamps
}

number_resamples <- 2000
resamps <- bootstrap_resamples(number_resamples, data = lst)

level <- 0.90
interval <- quantile(
  resamps, 
  probs = c((1 - level) / 2, (1 + level) / 2)
)

m <- mean(resamps)
sd <- sqrt(sum((m - resamps)^2)/ length(resamps))
```

Using the bootstrap method with `r number_resamples` resamples, we find that:

* The standard error is `r round(sd,3)`
* The approximate 90%-confidence interval for $\mu$ extends from `r round(interval[1],3)` to `r round(interval[2],3)`.

### Further Thoughts

* Given all of the picky assumptions in the parametric model, it may be best to go with the bootstrap.
* We might be able to use the data we have to research how to strike the best balance between the number of sampled households and the number of measurements at each household.  (For example, if a researcher could equally well two villages for two days each in not much more tme than it takes to visit one village for four days, could this provide more precise estimates?)

<!-- Christara does this app-->

```{r computation, echo=FALSE}
shinyAppDir(
  appDir = "computation",
  options = list(
    width = "100%", height = 550
  )
)
```
