---
title: "Notes on Confidence Intervals"
author: "CSC 400, Spring 2024"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
runtime: shiny
resource_files:
- computation/app.R
- mean/app.R
- sampling/app.R
- proportion/app.R
---

```{r setup, include=FALSE}
## this doucment is published to:
## https://homer.shinyapps.io/stoveteam/
library(tidyverse)
library(readxl)
library(shiny)
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

## Introducton

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras sed velit tincidunt, pharetra velit ac, faucibus libero. Praesent fermentum leo ac justo placerat, et interdum felis rhoncus. Phasellus mattis ullamcorper orci sit amet mattis. Vestibulum malesuada facilisis massa, et venenatis mi feugiat in. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Ut venenatis varius lacinia. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Etiam consectetur auctor pretium. Etiam id pretium purus. Ut mattis interdum nunc non lobortis. Morbi quis vulputate est. 




## Point Estimation and Sampling Variability

Integer ullamcorper, libero quis volutpat tempor, lacus augue viverra enim, ac convallis magna massa sed nulla. Mauris vehicula vitae justo nec ultricies. Nunc feugiat enim vitae quam pretium, in mollis nisl vehicula. Proin varius nulla elit, non aliquam urna aliquam quis. Maecenas malesuada erat convallis lectus blandit, eget efficitur ex maximus. Integer facilisis nisl eget felis semper, ac tempor diam pulvinar. Praesent bibendum augue enim, in egestas lectus mollis ut. Pellentesque tincidunt purus mi, eget sollicitudin leo vestibulum id. Etiam laoreet quam vitae metus ultrices, in faucibus ex dapibus. Vestibulum eu dictum sem, ac viverra velit. In ornare suscipit felis vitae ultrices. Morbi rhoncus mi id rhoncus hendrerit. Maecenas sed viverra diam, vitae tincidunt massa. Nullam ultrices, ligula et efficitur ullamcorper, risus mauris tincidunt tellus, id mollis urna mi porttitor leo. Nunc varius nulla semper ornare tincidunt. 

<!-- Eivory does this app-->

```{r sampling, echo=FALSE}
shinyAppDir(
  appDir = "sampling",
  options = list(
    width = "100%", height = 550
  )
)
```


## Confidence Intervals for a Proportion

Nulla interdum tellus vel urna blandit suscipit. Fusce facilisis ullamcorper orci, id aliquam ligula viverra vel. Mauris lorem lectus, luctus eget nisl non, lobortis cursus nisi. Cras facilisis non dui in ullamcorper. Nulla vel leo mollis, euismod lacus rhoncus, viverra elit. Etiam arcu tortor, placerat sit amet auctor semper, rutrum eget neque. Donec venenatis rutrum odio sed egestas. In libero neque, rhoncus at metus et, facilisis laoreet tellus. 

<!-- Finn does this app-->

```{r proportion, echo=FALSE}
shinyAppDir(
  appDir = "proportion",
  options = list(
    width = "100%", height = 550
  )
)
```


## Confidence Intervals for the Mean

 Fusce interdum consectetur placerat. In congue lobortis turpis eget sagittis. Vestibulum urna eros, commodo vel sapien rutrum, euismod pellentesque enim. Nam felis lacus, faucibus vel mattis id, placerat a mi. Cras lacinia risus vel magna varius vehicula. Sed sit amet hendrerit felis. Fusce ex sapien, viverra ut eros vitae, venenatis semper dolor. Nam volutpat tempus neque, nec suscipit sapien ornare sit amet.
 
<!-- Rhys does this app-->


```{r mean, echo=FALSE}
shinyAppDir(
  appDir = "mean",
  options = list(
    width = "100%", height = 550
  )
)
```


## Computation From the Data

### A Parametric Approach

Consider a population of $N$ households, where $N$ is some large number.  For each $i$, $1 \leq i \leq N$, let $\mu_i$ denote the mean per-capita daily use of firewood in the $i$-th household, measured as dry weight, in kilograms. Define:

$$\mu = \frac{\sum_{i=1}^{i=N}\mu_i}{N}.$$

Thus $\mu$ is the population mean of the mean household daily per-capita firewood use.  Our aim is to estimate $\mu$ and to compute a 90%-confidence interval for it.

Our data consists of several daily measurements for each household in a random sample from the population.  For a sample of $n$ households, let household $j$ in the sample (where $1 \leq j \leq n$) be measured $N_j$ times, and let the $i$-th measurement (where $1 \leq i \leq N_j$) on household $j$ be denoted by $X_{ji}$.  Thus the first household has measurements:

$$X_{11}, X_{12}, \ldots, X_{1N_1},$$

the second household has measurements:

$$X_{21}, X_{22}, \ldots, X_{2N_2},$$

and so on up to the $n$=th household which has measurements:

$$X_{n1}, X_{n2}, \ldots, X_{nN_n}.$$
An unbiased estimator for $\mu$ is:

$$\overline{\overline{X}} = \frac{\sum_{j=1}^{j=n}\bar{X}_j}{n},$$
where for each $j$,

$$\bar{X}_j = \frac{\sum_{i=1}^{i=N_j}X_{ji}}{N_j}$$
is the mean of the measurements for the $j$-th household in the sample.

Our estimator is subject, of course, to sampling variability, which is typically measured by the variance of the estimator.  The statement from the Aprovecho Research Center suggests using a pooled approach to computing this variance.  However, the pooled approach is only relevant when one may assume that the variance in daily measurements is the same for all households, and we see no reason why such an assumption is justified in this study.

Accordingly we propose that the variances be estimated separately.  Thus the variance for household $j$ is estimated in the standard way by the unbiased estimator:

$$\bar{\sigma^2}_j = \frac{\sum_{i=1}^{i=N_j}(X_{ji}-\bar{X}_j)^2}{N_j - 1},$$

which leads to the following unbiased estimator for the variance of $\overline{\overline{X}}$:

$$Var\left(\overline{\overline{X}}\right)=\frac{\sum_{j=1}^{j=n}\bar{\sigma^2}_j}{n^2}$$

The standardized random variable

$$Z = \frac{\overline{\overline{X}} - \mu}{\sqrt{Var\left(\overline{\overline{X}}\right)}}$$
has a distribution that approaches the standard normal distribution (mean 0, variance 1) as the sample size $n$ increases.  Therefore, for a "large-enough" sample of households, we may take the normal-based confidence interval

$$\left(\overline{\overline{X}} - z^*\sqrt{Var\left(\overline{\overline{X}}\right)}, \overline{\overline{X}} + z^*\sqrt{Var\left(\overline{\overline{X}}\right)}\right)$$
as an approximate 90%-confidence interval for $\mu$.  (Here, the multiplier z* denotes the 95th-percentile of the standard normal distribution.)

```{r}
summary_data <- read_excel(
  "st_files/For Review_Ret_Justa_July KPT Complete_KPT_4day (1).xlsx", 
  range = "J22:Q38"
)

names(summary_data) <-
  c(
    "per_cap_mean",
    "weighted_per_cap_mean",
    "sd",
    "var",
    "weighted_var",
    "days_measured",
    "df",
    "cov"
  )

compute_ci_2 <- function(data, level) {
  n <- nrow(data)
  sample_mean <- sum(data$per_cap_mean) / n
  sd_sample_mean <- sqrt(sum(data$var) / n^2)
  multiplier <- qnorm((1 + level) / 2)
  margin <- multiplier * sd_sample_mean
  interval <-
    c(
      lower = sample_mean - margin, 
      upper = sample_mean + margin
    )
  list(
    point_estimate = sample_mean, 
    sd = sd_sample_mean,
    interval = interval
  )
}

res <- 
  compute_ci_2(data = summary_data, level = 0.90)
```


For our data, results are as follows:

* The point-estimate for $\mu$ is `r round(res$point_estimate,3)`
* The standard error is `r round(res$sd,3)`
* The approximate 90%-confidence interval for $\mu$ extends from `r round(res$interval[1],3)` to `r round(res$interval[2],3)`.

However, the actual coverage-properties of these confidence intervals can be significantly different from their advertised confidence levels when the sample size is small, and especially when the population distribution is also skewed.

Our sample size is only 16, so we should see plot the sample to see whether there issome indication of skewness in the population.  Below is a density plot of the sample:

```{r}
ggplot(summary_data, aes(x = per_cap_mean)) +
  geom_density(fill = "skyblue") +
  geom_rug()
```

The sample has a strong right-skew, indicating that the population could be strongly right-skewed as well.  We might need to adopt a more robust approach to inference, here.

### The Bootstrap


```{r}
lst <- vector(mode = "list", length = nrow(summary_data))
for (i in 1:length(lst)) {
  vals <- read_excel(
    "st_files/For Review_Ret_Justa_July KPT Complete_KPT_4day (1).xlsx", 
    range = "I15:L15",
    sheet = paste0("HH", i, " Data"),
    col_names = FALSE
  ) %>% 
    as.matrix() %>% 
    t() %>% 
    .[,1]
  vals <- vals[!is.na(vals)]
  lst[[i]] <- vals
}

set.seed(2424)
resampled_means <- function(data) {
  n <- length(data)
  means <- numeric(n)
  for (i in 1:n) {
    grp <- data[[sample(1:n, size = 1)]]
    means[i] <-
      sample(grp, size = length(grp), replace = TRUE) %>% 
      mean()
  }
  means
}

## try it:

bootstrap_resamples <- function(m, data) {
  resamps <- numeric(m)
  for (i in 1:m) {
    resamps[i] <- mean(resampled_means(data = data))
  }
  resamps
}

number_resamples <- 2000
resamps <- bootstrap_resamples(number_resamples, data = lst)

level <- 0.90
interval <- quantile(
  resamps, 
  probs = c((1 - level) / 2, (1 + level) / 2)
)

m <- mean(resamps)
sd <- sqrt(sum((m - resamps)^2)/ length(resamps))
```

Using the bootstrap method with `r number_resamples` resamples, we find that:

* The standard error is `r round(sd,3)`
* The approximate 90%-confidence interval for $\mu$ extends from `r round(interval[1],3)` to `r round(interval[2],3)`.



<!-- Christara does this app-->

```{r computation, echo=FALSE}
shinyAppDir(
  appDir = "computation",
  options = list(
    width = "100%", height = 550
  )
)
```

## RStudio Filler

This R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately. 

To learn more, see [Interactive Documents](http://rmarkdown.rstudio.com/authoring_shiny.html).

### Inputs and Outputs

You can embed Shiny inputs and outputs in your document. Outputs are automatically updated whenever inputs change.  This demonstrates how a standard R plot can be made interactive by wrapping it in the Shiny `renderPlot` function. The `selectInput` and `sliderInput` functions create the input widgets used to drive the plot.

```{r eruptions, echo=FALSE}
inputPanel(
  selectInput("n_breaks", label = "Number of bins:",
              choices = c(10, 20, 35, 50), selected = 20),
  
  sliderInput("bw_adjust", label = "Bandwidth adjustment:",
              min = 0.2, max = 2, value = 1, step = 0.2)
)

renderPlot({
  hist(faithful$eruptions, probability = TRUE, breaks = as.numeric(input$n_breaks),
       xlab = "Duration (minutes)", main = "Geyser eruption duration")
  
  dens <- density(faithful$eruptions, adjust = input$bw_adjust)
  lines(dens, col = "blue")
})
```

### Embedded Application

It's also possible to embed an entire Shiny application within an R Markdown document using the `shinyAppDir` function. This example embeds a Shiny application located in another directory:

```{r tabsets, echo=FALSE}
shinyAppDir(
  system.file("examples/06_tabsets", package = "shiny"),
  options = list(
    width = "100%", height = 550
  )
)
```

Note the use of the `height` parameter to determine how much vertical space the embedded application should occupy.

You can also use the `shinyApp` function to define an application inline rather then in an external directory.

In all of R code chunks above the `echo = FALSE` attribute is used. This is to prevent the R code within the chunk from rendering in the document alongside the Shiny components.



