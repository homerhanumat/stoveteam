---
title: "Confidence Intervals:  Notes for Stoveteam, International"
author: "CSC 400, Spring 2024"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 2
  html_document:
    #toc: true
    #toc_float: true
    #toc_collapsed: true
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "docs", output_file = "index.html") })
#runtime: shiny
resource_files:
- computation/app.R
- mean/app.R
- sampling/app.R
- proportion/app.R
---

```{r setup, include=FALSE}
## this document is published to:
## https://homer.shinyapps.io/stoveteam/
library(tidyverse)
library(readxl)
library(shiny)
library(nlme)
library(shinyjs)
library(DescTools)
## for menas app:
##library(bslib)
##library(bsicons)
## knitr options:
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  out.width = "90%",
  fig.aligh = "center"
)

## utility function to get info on a single house:
get_house <- function(i) {
  vals <- read_excel(
    "st_files/For Review_Ret_Justa_July KPT Complete_KPT_4day (1).xlsx", 
    range = "I15:L15",
    sheet = paste0("HH", i, " Data"),
    col_names = FALSE
  ) %>% 
    as.matrix() %>% 
    t() %>% 
    .[,1]
  vals <- vals[!is.na(vals)]
  data.frame(
    house = rep(i, length(vals)),
    wood = vals
  )
}

## data frame with info on all houses.
## each row is a house on a day:
all_meas <- 
  map_dfr(factor(1:16), get_house) %>% 
  group_by(house) %>% 
  mutate(mean_percap = mean(wood)) %>% 
  mutate(deviation = wood - mean_percap)
```

# Introduction

In its [2023 Annual Report](https://www.stoveteam.org/2023-annual-report), Stoveteam International says that is commited to:

>"... reducing open-fire cooking, in Central America by producing and placing fuel-efficient cookstoves in homes in Guatemala, El Salvador, Nicaragua, and Honduras."

Stoveteam expects that these more fuel-efficient stoves will not only be safer for familes to use, but they can also reduce significantly the carbon-footprint of households where the stoves are in use and therefore, as the stoves are adopted more widely, reduce the carbon-footprint of the countries where Stoveteam works.  To that end, Stoveteam needs to conduct regular monitoring studies to estimate:

* the proportion of households provided with stoves that continue to use them after a set period of time;
* the mean, over all households that use the stoves, of the mean daily per-capita fuel-usage of the household.

Stoveteam wants its estimates of the above two quantities to be reliable to certain target levels of precision.  The aim of this document is recommend methods of estimation and sample sizes for each type of study so that the target levels of precision are likely to be achieved.

# Confidence Intervals for a Proportion

One of the regular Stoveteam monitoring tasks is to estimate the proportion $p$ of all households having the stoves that are still using their stoves.  A natural way to proceed is to take a random sample of $n$ households, count the number $X$ of them that are still using their stoves, and use the quantity

$$\hat{p} = \frac{X}{n}$$

as a *point-estimate* of $p$.

However, since it is a matter of chance which houses get into the sample, the estimator $\hat{p}$ is subject to sampling variability.  Therefore we require, in addition to a point-estimate of $p$, an interval of values that constitute reasonable possibilities, based on the data, for what $p$ might actually be.  Such an interval-estimate is known as a *confidence interval* for the unknown $p$.

## Level of Confidence

Statisticians have developed many methods for making confidence intervals for a population proportion based on a random sample from the population of interest.  No matter the method, each type of confidence interval carries with it a certain *level of confidence*.  The level of confidence associated with a method for making intervals is simply the probability, prior to taking a sample, that the interval that is eventually computed from the sample will contain $p$ between its upper and lower bounds---*covering* the parameter $p$, as statisticians like to say.

It is important to note that level of confidence is a statement about probability.  Once a sample has been taken and an interval is computed there are no "chances" left:  either the interval contains $p$ or it does not.  Nevertheless, because the interval was computed using a formula that produces intervals that cover $p$ a certain percentage of the time in repeated sampling---90% of the time if the level of confidence is 90%---then we are justified in having that particular level of confidence that the interval contains $p$.  If one makes, for example, a 90%-confidence interval for $p$ that extends from 0.53 to 0.63, then one can say:  "I don't know for sure whether the proportion of all households that still use their stoves lies within this interval, but I'm 90%-confident that it does."

It is important to distinguish the level of confidence of an interval-formula from its *nominal* level of confidence.  The level of confidence is the actual probability that an interval computed from this formula will contain $p$.  The nominal level of confidence is the probablity that it will contain $p$ given certain mathematical assumptions (often very precise) concerning the process that led to the data.  We always know the nominal level of confidence for any interval-formula that we plan to use.  Usually the actual level is not the same as the nominal level, though we hope that it will not differ from the nominal level by much.

### An Illustrative App

In the app below, the user can select:

* the actual value of $p$, the proportion of interest that is associated with this population;
* the size of random samples that will be taken from the population;
* the nominal level of confidence that confidence intervals should have;
* a method, i.e.: a particular type of interval-formula for making a confidence interval for the population proportion.

(**Note**:  The imaginary population is assumed to be very large---infinite, if you like.)

The user can then direct the app to take fifty random samples from the population, computing from each sample a confidence interval. The user can see which of these intervals cover $p$ and which do not. The process can be repeated as many times as the user likes, and the app will keep track of the percentage of times the intervals have covered $p$.  The longer the user goes on, the closer this percentage will come to the actual level of confidence of the interval.  Anytime the user likes he or she can start the proces over, selecting another method, or another sample size, or another level of confidence, or another value for $p$---or any combination of these.

**APP GOES HERE**

<!-- Finn does this app-->

```{r proportion, echo=FALSE, eval = FALSE}
shinyAppDir(
  appDir = "proportion",
  options = list(
    width = "100%", height = 550
  )
)
```

One lessson to be learned is that the higher the level of confidence, the wider (and therefore less "precise") the intervals are.  If one uses the app a great deal then he or she might eventually discover that at certain sample sizes and at certain values of $p$ some methods perform poorly in the sense that their actual and nominal lovels of confidence differ considerably.

## Recommended Method


As we have said, there are many methods for computing a confidence interval for a population proportion $p$.  The document *CDM-EB67-A06-GUID* appears to assume (see Equation 7 near the bottom of page 71) the use of what is known as the *Wald Interval*:

$$\left(\hat{p} - z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}, \hat{p} + z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}\right),$$
where:

* $\hat{p} = X/n$ is the *sample proportion* (the number $X$ of successes in the sample divided by the sample size $n$);
* $z^*$ is the percentile of the standard normal distribution appropriate to the desired level of confidence (for a 90%-confidence interval this is approximately `r round(qnorm(0.95), 3)`).

**Note**:  Equation 7 actually included an attempt at the *finite population correction factor* but got the factor slightly wrong.  The interval with correction factor should be:

$$\left(\hat{p} - z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}\sqrt{\frac{N-n}{N-1}}, \hat{p} + z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}\sqrt{\frac{N-n}{N-1}}\right),$$
where $N$ is the number of households that were provided with stoves.

Wald intervals don't have the best statistical properties:  for smaller sample sizes, and for population proportions that are near 0 or near 1, the nominal level of confidence of the intervals can differ significantly from their actual levels.  In the situation of our study the sample size is not very large, so we will recommend a more sophisticated method.

A Wilson interval with a finite population correction would be a fine choice, as it has been demonstrated to have good statisitcal properties at small sample sizes, provided that $p$ is not very close to 0 or 1.  (See Seung-Chun Lee, "Confidence Intervals for a Proportion in Finite-Population Sampling", *Communications of the Korean Statistical Society*, 2009, Vol. 16, no. 3, pp. 501-509.) The formula for the interval is the rather daunting expression:

$$\tilde{p}^*\pm z^*\frac{\sqrt{1 - f^*}}{\tilde{n}^*}\sqrt{n\hat{p}(1-\hat{p}) + (1-f^*)(z^*)^2/4},$$
where:

* $n$ is the sample size;
* $\hat{p}$ is the sample proportion (number of successes $X$ in the sample divided by $n$);
* $z^*$ is the percentile of the standard normal distribution that corresponding to the desired level of confidence (approximately 1.645 for a 90%-confidence interval);
* $f^* = (n-1)/(N-1)$, where $N$ is the size of the population from which the sample is drawn;
* $\tilde{n}^*= n + (1-f^*)(z^*)^2$;
* $\tilde{p}^* = \left(X + (1-f^*)(z^*)^2/2\right)/\tilde{n}^*$.

```{r}
wilson_corrected <- function(x, n, N, level) {
  z <- qnorm((1 + level) / 2)
  p_hat <- x / n
  f <- (n - 1) / (N - 1)
  n_t <- n + (1 - f) * z^2
  p_t <- (x + (1 - f) * z^2 / 2) / n_t
  margin <- z*sqrt(1-f)*sqrt(n*p_hat*(1-p_hat)+(1-f)*z^2/4)/n_t
  interval <- c(
    lower = p_t - margin,
    upper = p_t + margin
  )
  list(
    ci = interval,
    margin = margin
  )
}

res_w <- wilson_corrected(
  x = 37,
  n = 65,
  N = 500,
  level = 0.90
)
```


For example, if we sample $n=65$ households at random from a population of $N=500$ households that are provided with stoves and we find that $X=37$ are still using their stoves, then a 90%-confidence interval for $p$, the proportion of all 500 households that still use their stoves, would extend from `r round(res_w$ci[1], 3)` to `r round(res_w$ci[2], 3)`, with the margin of error being `r round(res_w$margin, 4)`.  (By happenstance this turns out to be nearly the same as the corrected Wald interval, but we'll stick with it in spite of the extra effort.)

## Sample Size

Our estimate of $p$ is required to have a minimum *precision* of 0.10.

From Section 12(a) of the document *CDM-EB50-A30-STAN* we can infer a definition of precision:

>As a relative unit when the parameter of interest is a proportion (or a percentage) For example, ± 10 per cent in relative units means that the interval around a proportion value of 70 per cent is 63 per cent to 77 per cent. A proportion can describe either of the two possible scenarios of the success rate (p) or the failure rate (1−p), for example (i) cookstove still operational or (ii) cookstove no longer operational. The project participants or the coordinating/managing entity may use the larger of the two proportions in the sample size calculation, that is (p) or (1−p), in any of the monitoring periods during the crediting period without having to revise the monitoring plan.

We see that precision should be calculated as:

$$\text{precision} = \frac{\text{margin of error}}{\max(\hat{p}, 1 - \hat{p})}.$$

Thus in our example where we sample $n=65$ households at random from a population of $N=500$ households that are provided with stoves and find that $X=37$ are still using their stoves, we have $\hat{p} = 37/65 \approx 0.569$ so the maximum of $\hat{p}$ and $1-\hat{p}$ is 0.569 and the precision would be:

$$\frac{\text{margin of error}}{0.569} \approx \frac{0.0927}{0.569} \approx 0.163,$$
a good bit higher than our target.  We will have to take a larger sample!

We would like to derive a formula for the sample size $n_d$ required for the resulting confidence interval to have a precision not exceeding 0.10.  In order to derive the formula we must assume a fairly simple fornula for the margin of error, so we'll use the margin of error for Wald intervals, even if the confidence intervals we actually make are based on another method.

So we are looking for a value of $n_d$ such that:

$$\frac{z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n_d}}\sqrt{\frac{N-n_d}{N-1}}}{\max(\hat{p}, 1 - \hat{p})} \leq 0.10.$$
Of course, we won't know $\hat{p}$ until we actually take our sample, so we need to replace occurences of $\hat{p}$ in the above inequalities with reasonable approximations.

Now the precision is larger (i.e., "worse") when the denominator $\max(\hat{p}, 1 - \hat{p})$ is smallest, and this occurs when $\hat{p} = 0.50$.  Also the precision is larger when the numerator is largest, and this occurs when $\hat{p}(1-\hat{p})$ is largest, and this also occurs when $\hat{p} = 0.50$.  Therefore if we replace $\hat{p}$ with 0.50, we get an inequality

$$\frac{z^* \sqrt{\frac{0.5(1 - 0.5)}{n_d}}\sqrt{\frac{N-n_d}{N-1}}}{\max(0.5, 1 - 0.5)} = \frac{z^*\sqrt{N-n_d}}{\sqrt{n_d(N-1)}}\leq 0.10.$$
which, when solved for $n_d$, results in a sample size that is conservative in the sense that it is liable to be larger than needed.

Solving the inequality for $n_d$, we get:

$$n_d \geq \frac{(z^*)^2N}{0.10^2(N-1) +(z^*)^2}.$$

This agrees with the formula suggested on page 28 of *CDM-EB50-A30-STAN*, when one replaces the $p$ in that formula with 0.50.

Applying the above formula: if we plan to make a 90%-confidence interval and the size of the population is $N=500$, then we find that the sample size should be at least

$$\frac{1.645^2\times 500}{0.10^2\times 499 +1.645^2} \approx 176.$$


# Confidence Intervals for the Mean

A second Stoveteam monitoring task is to estimate the mean, over all households in the population of households that use their stoves, of the mean daily per-capita usage of wood fuel.  The goal now is to estimate a population mean with the mean of a random sample of numbers, rather than a population proportion from a sample of answers to a Yes/No question; nevertheless the need for an interval estimate persists, and the fundamental concepts of confidence intervals still apply.

We will address the specific monitoring task in the next section.  In this section, let's just look at confidence intervals for the mean of a population.  To keep things as simple as possible, we'll assume that the population is quite large in comparison to the sample size; hence there will be, for now, no talk of finite population correction factors.

Suppose that the population is normally distributed with an unknown mean $\mu$ and an unknown standard deviation $\sigma$, and that we plan to take a random sample

$$X_1, X_2, \ldots, X_n$$
of size $n$ from the population.  We can estimate $\mu$ with the sample mean:

$$\bar{X} = \frac{\sum_{i=1}^n X_i}{n}.$$

We don't know the population standard deviation, but statistical theory suggests that it can be estimated by the *sample standard deviation*:

$$\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n-1}}.$$
It turns out that the following random quanitty known as the $t$-statistic

$$T = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{n}}$$

has a known probability distribution, namely the $t$-distribution with $n-1$ degrees of freedom.  From this fact it is possible to derive a formula for a confidence interval for $\mu$ of any desired level of confidence.  The formula is as follows:
 
 $$\left(\bar{X} - t^*_{n-1}\frac{\hat{\sigma}}{\sqrt{n}}, \,\bar{X} + t^*_{n-1}\frac{\hat{\sigma}}{\sqrt{n}}\right),$$
 
where $t^*_{n-1}$ is the percentile of the $t$-distribution associated with the nominal level of confidence.  Assuming normality of the population, the nominal level of confidence is exactly the true level of confidence.

Differences between nominal and actual levels of confidence arise when the above formula for confidence intervals is applied when the population is not normally distributed.  Statistical theory guarantees that, as sample size $n$ increases to infinity, the actual level of confidence converges to the nominal level, but when sample size is small and the underlying population departs from normal in significant ways---especially if it is strongly skewed or has significant outliers---then the differences can be large enough to worry us.

### An Illustrative App

The following app permits the reader to see the effect of departures from normality on the actual level of confidence, at various sample sizes and various levels of confidence.

The user can choose between four different populations:

* normally distributed;
* skewed-right (a member of the *exponential* family of distributions);
* very skewed right (a member of the *Pareto* family of distributions);
* a population with a substantial group of outliers (a member of the *mixed-normal* family of distributions).

The user will be able to see a density plot of the selected population and a statement of its mean $\mu$.

The user may also choose the sample size and the nominal level of confidence of the intervals made from the samples to be taken.

When the user asks to begin simulation, the app will, at the user's discretion, draw either 50 or 100 random samples from the population, and the user will be shown a graph that shows the resulting confidence intervals for $\mu$ and whether or not each interval covered $\mu$.  The app will also keep a running total of the number of samples taken and the percentage of intervals that covered $\mu$.  This percentage is an estimate of the actual level of confidence---an estimate that converges to the actual level as the number of samples taken increases.

Pressing a Start Over button, the user can make new choices for population, smaple size, and nominal level, and then begin a new simulation.

For the normal population, the actual and nominal confidence levels are the same, at any sample size. For other populations, expect to see differences between nominal and actual levels--in some cases even when the sample size is fairly large.

**APP GOES HERE**

 
<!-- Rhys does this app-->


```{r mean, echo=FALSE, eval = FALSE}
shinyAppDir(
  appDir = "mean",
  options = list(
    width = "100%", height = 550
  )
)
```





# Confidence Intervals for the Mean of Household Means

We now turn to the specific task at hand:  estimating the mean of mean household per-capita daily usage of wood fuel.

## Our Recommended Approach

We imagine a population of households.  Each household $h$ in the population has a mean per-capita daily usage of wood, which we will denote $\mu_h$.  Of course $\mu_h$ varies from one household to another.  Let $\mu$ denote the mean of all these mean per-capita daily usages.  $\mu$ is the number that we are trying to estimate and for which we desire to form a confidence interval, based upon the data we have collected.  (**Note:** See the Appendix for more on why this is the appropriate parameter.)

We make the assumption that the population of $\mu_h$-values is normally distributed, with mean $\mu$ and some unknown standard deviation $\sigma_b$.  (The subscript $b$ calls to mind that the mean daily per-capita usage varies *between* households, i.e., from one household to another.)

We select $n$ households at random from our population, and visit each household.  Let $i$ be any number from 1 to $n$, and consider household $i$ (the $i$-th household selected for study).  When we arrive at household $i$, we are of course not provided its $\mu_i$ value.  Instead we must estimate $\mu_i$ by measuring the per-capita usage for each of $J_i$ days (in the study conducted, $J_i$ is sometimes 3 and sometimes 4).  We get $J_i$ measurements:

$$X_{i1}, X_{i2}, \ldots, X_{iJ_i}.$$
We compute the mean of these measurements:

$$\bar{X}_i = \frac{\sum_{j=1}^{J_i}X_{ij}}{J_i}.$$
to estimate $\mu_i$.

Now there is variability in this estimator $\bar{X}_i$, because the actual per-capita usage in a single household varies from day to day.  We make an important simplifying assumption, namely:  that the one-day per-capita usage for the household is normally distributed, with mean $\mu_i$ and some unknown standard deviation $\sigma_w$. (The subscript $w$ in the notation calls to mind that the variability is "within" a fixed household from day to day.) Importantly, we are assuming that $\sigma_w$ is the *same* for all households in the population.

We would then estimate $\mu$ by computing the mean of the $n$ household means:

$$\overline{\overline{X}} = \frac{\sum_{i=1}^n \bar{X}_i}{n}$$

Note that there are *two* sources of variability in the above estimator:

* variability in mean daily per-capita usage from one household to another, expressed by the unknown $\sigma_b$;
* variability in per-capita usage from day to day, *within* a fixed household, expressed by the unknown $\sigma_w$.

In fact, probability theory tells us that the distribution of $\overline{\overline{X}}$ is normal, with mean $\mu$ and standard deviation:

$$\frac{\sqrt{\sigma^2_b +\frac{\sigma^2_w\left(1/J_1+\ldots+1/J_n\right)}{n}}}{\sqrt{n}}.$$

Now let us turn to the task of making confidence intervals for $\mu$.  To this end, define the *between-sum of squares*:

$$SS_b = \sum_{i=1}^n \left( \overline{\overline{X}} - \bar{X}_i \right)^2.$$

A bit of standard statistical theory tells us that if all households were measured for the same number $J$ of days, then the random quantity:

$$T = \frac{\overline{\overline{X}} - \mu}{\sqrt{\frac{SS_b}{n(n-1)}}}$$
would have a t-distribution, with degrees of freedom equal to $n-1$.  (Interestingly, the degrees of freedom are independent of $J$.)

We now find ourselves in the very same situation descrobed in the previous section, where one must estimate the mean of of normall-distributed population.  Accordingly we can derive, in the manner familiar to statisticians, the following formula for confidence intervals:

$$\left(\overline{\overline{X}} - t_{n-1}^*\sqrt{\frac{SS_b}{n(n-1)}},\, \overline{\overline{X}} + t_{n-1}^*\sqrt{\frac{SS_b}{n(n-1)}}\right),$$

where $t_{n-1}^*$ is the quantile of the t-distribution with $n-1$ degrees of freedom that is appropriate for the desired level of confidence.

The above formula assumes that the population-size is "infinite".  In fact, if the population is of some finite size $N$, then to obtain an exact interval one would employ the *finite-poulation correction factor*

$$\sqrt{\frac{N-n}{N-1}}$$

to the margin of error, obtaining:

$$\left(\overline{\overline{X}} - t_{n-1}^*\sqrt{\frac{SS_b}{n(n-1)}}\sqrt{\frac{N-n}{N-1}},\, \overline{\overline{X}} + t_{n-1}^*\sqrt{\frac{SS_b}{n(n-1)}}\sqrt{\frac{N-n}{N-1}}\right).$$
The correction factor need only be incorporated when the sample size $n$ is a substantial fraction of the size of the population.

Clearly, though, there is an obstacle to using either of the above intervals, since in our study not all households were measured for the same number of days:  the number of days was sometimes 3 and sometimes 4.  Hence the random variable $T$ will not, for us, have an exact $t$-distribution.  The nominal level of our confidence intervals won't be exactly equal to their actual level.

We might try to address the issue by taking a conservative approach.  What if we we were to simply throw away the final measurement for any day that had four measurements?  Then the number $J$ above could be set to three, and from the remaining data we could compute exact confidence intervals.

```{r}
compute_ci_1 <- function(data, level, use = NULL, N = NULL) {
  if (is.null(use)) {
    house_info <-
      data %>% 
      group_by(house) %>% 
      summarize(mean = mean(wood))
  } else {
    house_info <-
      data %>% 
      group_by(house) %>% 
      slice_head(n = use) %>% 
      summarize(mean = mean(wood))
  }
  n <- nrow(house_info)
  multiplier <- qt((1 + level) / 2, df = n - 1)
  sample_mean <- mean(house_info$mean)
  ss_between <- sum((sample_mean - house_info$mean)^2)
  margin <- multiplier * sqrt(ss_between / (n * (n - 1)))
  if (!is.null(N)) {
    ## apply finite-population correction factor
    margin <- margin * sqrt((N - n) / (N - 1))
  }
  list(
    point_estimate = sample_mean, 
    margin = margin,
    interval = c(
      sample_mean - margin,
      sample_mean + margin
    )
  )
}

## try it out:
res <- compute_ci_1(
  data = all_meas, 
  level = 0.90,
  use = 3
)

res_all <- compute_ci_1(
  data = all_meas, 
  level = 0.90
)

res_all2 <- compute_ci_1(
  data = all_meas, 
  level = 0.90,
  N = 500
)
```


When we throw out the fourth-day measurements and make a confidence interval based on the above formula, where each household has three measurements, we find that:

* The point-estimate of $\mu$ is `r round(res$point_estimate, 3)`.
* The 90%-confidence interval extends from `r round(res$interval[1], 3)` to `r round(res$interval[2], 3)`.
* The margin of error for the above interval is `r round(res$margin, 3)`.


On the other hand, if we bite the bullet and keep all of the measurements---simulation studies to be covered in a subsequent section suggest that there is no great harm in doing this---we find that:

* The point-estimate of $\mu$ is `r round(res_all$point_estimate, 3)`.
* The 90%-confidence interval extends from `r round(res_all$interval[1], 3)` to `r round(res_all$interval[2], 3)`.
* The margin of error for the above interval is `r round(res_all$margin, 3)`.

As an example of the use of the finite population correction factor, suppose that there are 500 households in the population.  Then when we keep all of the data and use the correction factor the interval shrinks a bit, extending from `r round(res_all2$interval[1], 3)` to `r round(res_all2$interval[2], 3)`.

## A Caution

Recall that one of our assumptions was that the underlying population of mean per-capita household usages was normally distributed.  In practice, no population distribution is ever exactly normal.  We can take some comfort from statistical theory, which assures us that even if the population is not normal, the actual coverage-properties of our confidence intervals converge to the advertised coverage-properties as sample size increases to infinity. Nonetheless, when the sample size is small and the population distribution differs significantly from normal (especially if it is skewed in one direction or another) then there may be a significant difference between actual and advertised performance.

Our sample size is only 16 households--somewhat small--so we should plot the sample to see whether there any indication of skewness in the population.  Below is a density plot of the sample:

```{r}
house_info <-
  all_meas %>% 
  group_by(house) %>% 
  summarize(mean = mean(wood), J = n())
ggplot(house_info, aes(x = mean)) +
  geom_density(fill = "skyblue") +
  geom_rug()
```

The sample has a significant right-skew, indicating that the population could be significantly right-skewed as well.  This is another reason to believe that the nominal and actual level of confidence of our intervals will differ.

In future studies it would be prudent to take a larger sample of households, perhaps 30 or so.  (We are likely to take a much larger sample anyway, in order to meet the required precision:  see the later section on sample size.)

## The Random-Effect Approach

Our problem may also be viewed as a very special case of a linear random-effects model.  Viewing the problem in this way and using the R-package **nlme**, we get the following results:

```{r}
mod <- nlme::lme(wood ~ 1, random = ~ 1 | house, data = all_meas)
res_lme <- intervals(mod, level = 0.90)
```


* The point estimate of $\mu$ is `r round(res_lme$fixed[1,"est."], 3)`
* The 90%-confidence interval extends from `r round(res_lme$fixed[1,"lower"], 3)` to `r round(res_lme$fixed[1,"upper"], 3)`.

We prefer the previous approach, as it is simple enough for the users to compute on their own without having to acquire familiarity with statistical software.


## Simulation Studies

We have conducted some simulation studies to get an idea of the effect of the likely skewness of the population and the non-uniformity of the sample size from household to household on the performance of our formula for confidence intervals.  In our simulations the parametric confidence-intervals constructed using all of the data and having a 90% nominal level of confidence actually covered the mean about 89% of the time.  Therefore it should be acceptable to use the suggested interval formula, keeping all of the data.  (In the computations for the mixed-effects model this is, in fact, always done.)

### An Illustrative App

The following app allows readers to conduct some simulations of their own.

The app constructs a hypothetical infinite population that has what statisticians call a *mixed-normal* distribution, i.e.: the population is a combination of two normally distributed groups. (You can think of them as two groups of households:  Group One tending to use not so much wood per-dapita in a day, and Group Two using more.). The user chooses the mean and the standard deviation of both groups, as well as the proportion of the total population that is Group One (the rest of the population being Group Two).

The per-capita usage each day in each household in the population is itself normally distributed, with a mean equal to the household mean and a standard deviation that the user can set.  (This standard deviation applies to all of the households in the population.)

The user can set a nominal level of confidence for the intervals that will be made for $\mu$, the mean of household means.

The user can also specify how many measurements to make at each household in the sample, by entering text into the relevant text-input field.  The entry must be in the form of ordered pairs separated by commas.  For example, the user might enter:

>(4, 8), (3, 7), (5, 1)

This means that we plan to sample $8+7+1 = 16$ households at random from the populations, and

* 8 houses will be studied for 4 days each;
* 7 houses will be studied for 3 days each;
* 1 house will studied for 5 days.

Thus the text-entry determines the size of the samples that will be taken.

Finally, the user can specify how many random samples will be taken in the simulation.  When the user presses the provided action-button the simulation commences:  the specified number of random samples is taken, and from each random sample a confidence interval for $\mu$ is made.

When the simulation is complete the main panel of the app will show two tabs:

* In the first tab is a density plot of the mixed-normal population, with a statement of its mean.  (This is the $\mu$ that our confidence intervals aim to cover.)
* In the second tab there is a plot of the distribution of the $t$-statistics associated with the simulated samples, and for comparison a plot of the $t$-distribution with degrees of freedom equal to one less than the sample size.  The latter is the distribution used to obtain critical values for our confidence intervals.  The more closely the plot of simulated $t$-statistics resembles the plot of the $t$-distribution, the closer the actual level of confidence should be to the nominal level.  Underneath is a statement of the proportion of the time the simulated intervals covered $\mu$.  This estimates the actual level of confidence.

The user may start over and set new specifications for the simulation.

**APP GOES HERE**

<!-- Christara does this app-->

```{r computation, echo=FALSE}
# shinyAppDir(
#   appDir = "computation",
#   options = list(
#     width = "100%", height = 550
#   )
# )
```

**Note**:  If the two groups are given the same mean and same standard deviation, then the resulting population is normally distributed, with that common mean and that common standard deviation.  If in addition the user specifies that all households should be studied for the same number of days---e.g., by entering something like `(4, 16)` to indicate that we want to sample 16 households and study each of them for four days---then all of the mathematical assumptions of our confidence-interval method are met, and the actual and nominal levels of confidence are the same.


## The Bootstrap

The *bootstrap* is a general strategy for statistical inference that be implemented with little or nothing in the way of modeling assumptions concerning the process that generates one's data, other than that the sampling is done in a controlled and quantifiable random fashion.  It relies on the fact that, when the data are sampled randomly from the population, then its distribution is liable to resemble approximately the distribution of the population itself.

In our situation we can design a version of the bootstrap that retains the assumptions that the daily per-capita measurements in each household are normally distributed with a standatd deviation that does not depend on the household, but let go of the assumption that the mean per-capita usages for the population of households has a normal distribution.

The recipe for a 90%-confidence interval is as follows:

1. Use the deviations of the daily measurements from the mean of their respective households to estimate $s_w$, the standard deviation of the measurments within any fixed household.
1. Select a reasonably large number $B$ (we will choose $B=1999$), and perform the following process $B$ times:
    * Sample 16 households at random and with replacement, from the the sixteen households in the sample.  (This is called "re-sampling".)
    * For each household $h_i$in the re-sample, $1 \leq i \leq 16$, generate $J_i$ random values from a normal distribution with mean $\mu_i$ (the mean per-capita usage for household $h_i$) and standard deviation $s_w$, and compute the mean (denoted $\bar{x}^*_i$) of these values.  These are the "re-sampled means".
    * Compute the mean $\bar{\bar{x}}^*$ of these re-sampled means.

We now have $B$ means.  We compute the 5th and 95th percentiles of this set of numbers, and use them as the lower and upper bounds respectively of a confidence interval for $\mu$.  Bootstrap theory guarantees that as the number of households sampled increases, the probability that such an interval contains $\mu$ converges to 90%.


```{r bootstrap, cache = TRUE}
## set a seed:
set.seed(5656)
## estimate within-household variance:
sd_w <-
  all_meas %>% 
  group_by(house) %>% 
  mutate(house_mean = mean(wood)) %>% 
  summarize(n = n(), sum_sq = sum((wood - house_mean)^2)) %>% 
  mutate(deg_freedom = n - 1) %>% 
  summarize(var = sum(sum_sq * deg_freedom) / sum(deg_freedom)) %>% 
  pull(var) %>% 
  sqrt()

## get household means and measurement-numbers:
house_info <-
  all_meas %>% 
  group_by(house) %>% 
  summarize(mean = mean(wood), J = n())

## get resamples
B <- 1999
resamples <- numeric(B)
n <- nrow(house_info)
houses <- numeric(n)
for (i in 1:B) {
  resampled_houses <-
    house_info[sample(1:n), size = n]
  resampled_mean <-
    resampled_houses %>% 
    mutate(mean_rs = mean(rnorm(J, mean = mean, sd = sd_w))) %>% 
    summarize(xbar = mean(mean_rs)) %>% 
    pull(xbar)
  resamples[i] <- resampled_mean
}

## compute a percentile bootstrap interval:
level <- 0.90
interval <- quantile(resamples, probs = c(0.05, 0.95))
```

We find that:

* The approximate 90%-confidence interval for $\mu$ extends from `r round(interval[1],3)` to `r round(interval[2],3)`.

The fact that the bootstrap with its relatively loose assumptions concerning the population structure yields about the same confidence interval as the two "parametric" methods covered previously should provide further assurance that it is reasonable to use the confidence-interval formulas that we have derived.

## Sample Size

```{r}
xbar <-
  all_meas %>% 
  group_by(house) %>% 
  summarize(xbarh = mean(wood)) %>% 
  ungroup() %>% 
  summarize(mean = mean(xbarh)) %>% 
  pull(mean)
ss_b <-
  all_meas %>% 
  group_by(house) %>% 
  summarize(xbarh = mean(wood)) %>% 
  summarize(ssb = sum((xbarh - xbar)^2)) %>% 
  pull(ssb)
tsq <- qt(0.95, df = 15)^2
N <- 500
n <- 16
n_d <- (tsq*ss_b*N/(n-1)) / (0.10^2*xbar^2*(N-1)+tsq*ss_b/(n-1))
```

The document *CDM-EB50-A30-STAN* specifies that confidence interval for a mean have a precision of no more than 10%.  In Section 12(b) of the document, precision for a confidence interval for the mean is defined as the margin of error divided by the sample mean (multiplied by 100 to convert to a percentage).

For the previous study, the margin of error for the confidence interval with the finite population correction factor was about `r round(res_all2$margin, 3)` and the sample mean was about `r round(xbar, 3)`, so the precision was `r round(res_all2$margin / xbar, 3)`. Clearly we will want larger sample sizes in the future in order to attain the required precision.

For the desired precision, from a population of size $N$ we would require a sample size $n_d$ such that:

$$\frac{t_{n_d-1}^*\sqrt{\frac{SS_b}{n_d(n_d-1)}}\sqrt{\frac{N-n_d}{N-1}}}{\bar{\bar{x}}} \leq 0.10.$$

Of course we have not taken the new sample yet, so we do not yet know what $SS_b$ and $\bar{\bar{x}}$ are.  Hence we won't be able to determine $n$ exactly. However:

* for $SS_b/(n_d-1)$ we will substitue an estimate, namely: the value for $SS_b/(n-1)$ that we obtained in the pilot study with sample size $n$;
* for $t_{n_d-1}^*$, will use $t_{n-1}^*$ (which is a bit larger, tending to result in an overestimate of $n_d$);
* for $\bar{\bar{x}}$ we will substitute the corresponding value that we got from the pilot study.

Our inequality now looks like:

$$\frac{t_{n-1}^*\sqrt{\frac{SS_b}{n_d(n-1)}}\sqrt{\frac{N-n_d}{N-1}}}{\bar{\bar{x}}} \leq 0.10.$$


We solve the above inequality for $n_d$, getting:

$$n_d \geq \frac{(t_{n-1}^*)^2\frac{SS_b}{n-1}N}{0.10^2(\bar{\bar{x}})^2(N-1)+(t_{n-1}^*)^2\frac{SS_b}{n-1}}.$$
For example, suppose that in our future study the size of the population of households using their stove is $N=500$.  From the previous study we had:


* $n = 16$;
* $SS_b \approx$ `r round(ss_b, 3)`;
* $\bar{\bar{x}} \approx$ `r round(xbar, 3)`;
* $t^*_{n-1} \approx$ `r round(qt(0.95, df = 15), 3)`.

Substituting into our formula, we find $n_d \geq$ `r round(n_d, 3)`, so a sample size of at least `r ceiling(n_d)` households is recommended.

Even though our computation of $n_d$ involves a bit of overestimation due to the use of $t^*_{n-1}$, the precision of the estimate based on the new sample, depending as it does on the random quantities $SS_b$ and $\bar{\bar{x}}$, will itself be subject to chance variation.  We want to minimize the chance of that precision coming out too large.  Therefore we might wonder:  what are the chances of meeting the target precision, at various sample sizes?

In order to shed some light on this question we conducted a simulation study.  We imagined a normally distributed population of size $N=500$, and performed the following procedure 10,000 times:

* Sample 16 items from the population.
* Use this sample to compute $n_d$ as per the formula above.
* Add to $n_d$ some "padding" amount $s$ for safety;
* Take a random sample of size $n_d + s$ from the population.
* Compute the precision associated with this sample for a 90%-confidence interval.

We performed the above procedure for the following values $s$:  0, 10, 20 and 30.  For each $s$, we computed the proportion of times that the precision was less than the target value of 0.10.  The results are shown in the table below:


```{r precision-sim, cache = TRUE}
set.seed(2020)
precision_sim_2 <- function(reps, mu, sd, n, N, 
                            level, target, addons) {
  probs <- numeric(length(addons))
  for (j in 1:length(probs)) {
    nds <- numeric(reps)
    ps <- numeric(reps)
    for (i in 1:reps) {
      samp <- rnorm(n, mean = mu, sd = sd)
      v <- var(samp)
      xhat <- mean(samp)
      t <- qt((1 + level) / 2, df = n - 1)
      n_d <- ceiling(t^2*v*N / (target^2*xhat^2*(N-1)+t^2*v))
      n_d <- n_d + addons[j]
      nds[i] <- n_d
      samp_new <- rnorm(n_d, mean = mu, sd = sd)
      stdev <- sd(samp_new)
      xhat_new <- mean(samp_new)
      f <- (N - n_d) / (N - 1)
      t_new <- qt((1 + level) / 2, df = n_d - 1)
      ps[i] <- t_new * stdev * sqrt(f) / (sqrt(n_d) * xhat_new)
    }
    probs[j] <- mean(ps <= 0.10)
  }
  df <- data.frame(
    addons,
    probs
  )
  names(df) <- c("s", "Pr(precision <= 0.10")
  df
}

res_p2 <- precision_sim_2(
  reps = 10000,
  mu = 5,
  sd = 2,
  n = 16,
  N = 500,
  level = 0.90,
  target = 0.10,
  addons = seq(0, 30, by = 10)
)
```

```{r}
knitr::kable(res_p2)
```


Reflecting on the above table, one might choose to increase the future sample size---from `r ceiling(n_d)` to `r ceiling(n_d) + 20`, perhaps.


**Note**: If we choose not to employ a finite population correction factor, the the formula simplifies to:

$$n_d \geq \frac{(t_{n-1}^*)^2\frac{SS_b}{n-1}}{0.10^2(\bar{\bar{x}})^2}.$$


**Note**:  As a rough rule of thumb, the margin of error is inversely proportional to the square root of the number of households sampled.  Thus in order to cut the marign of error in half (for example)  we would have to sample roughly four times as many households as we did in the pilot study.

## Number of Measurements at Each Household

It is unclear whether the guiding documents require a minimum number of days of study at the households in the sample.  If there are no hard-and-fast requirements then we might consider spending fewer than four days at each household, provided that doing so would enable researchers to select and visit a larger number of households.

Consider the formula for the standard deviation of the estimator for $\mu$:

$$SD\left(\bar{\bar{X}}\right) = \frac{\sqrt{\sigma^2_b +\frac{\sigma^2_w\left(1/J_1+\ldots+1/J_n\right)}{n}}}{\sqrt{n}}.$$
When all households are visited for the same number $J$ of days, this reduces to:

$$SD\left(\bar{\bar{X}}\right) = \frac{\sqrt{\sigma^2_b +\sigma_w^2 /J}}{\sqrt{n}}.$$
In order to improve the precision of an estimate of $\mu$ we want this standard deviation to be as small as possible.  As long as $\sigma_w$ is not a great deal larger than $\sigma_b$, increasing the number of households $n$ will do much more to decrease $SD\left(\bar{\bar{X}}\right)$ than increasing $J$ would.

A random-effects model actually provides estimates of both $\sigma_b$ and $\sigma_w$.  We used these estimates to compute the estimated precision of 90%-confidence intervals that are based on various sample sizes $n$ and number of measurements $J$.  Some of the results are shown in the table below:


```{r}
mod <- nlme::lme(wood ~ 1, random = ~ 1 | house, data = all_meas)
sds <- as.numeric(VarCorr(mod)[, 2])

pred_prec <- function(model, J, n, level) {
  sds <- as.numeric(VarCorr(model)[, 2])
  sb <- sds[1]
  sw <- sds[2]
  xbar <- model$coefficient$fixed
  se_xbar <- sqrt(sb^2 +sw^2/J) / sqrt(n)
  t <- qt((1 + level) / 2, df = n - 1)
  t * se_xbar / xbar
}

df_prec <- data.frame(
  households = c(16, 65, 80, 80),
  measurements = c(1000, 4, 2, 1)
) %>% 
  mutate(precision = pred_prec(
    model = mod,
    J = measurements,
    n = households,
    level = rep(0.90, 3)
  ))
```

```{r}
knitr::kable(df_prec)
```

From this table we see:

* If we keep our sample size at 16 but measure each household for a thousand days, we would expect a precision scarcely smaller than the precision in the study we actually conducted with 3-4 measurements per household.  **If $J$ is already large, increasing it scarcely helps!**
* Increasing the sample size to 65 (the level we recommended in the section on sample size) cuts the precision about in half.  This requires $4 \times 65 = 280$ measurement days.
* Increasing the sample size to 80 and spending just two days at each household results in a slightly better precision, with only 160 measurement days required.
* Spending just one day at each of 80 households should do just about as well as the 280 measurement-day scheme.

Accordingly, if it is permitted to reduce the number of measurements at each household, then Stoveteam should consider doing so.  (Of course, in striking the optimal balance between $n$ and $J$ one must take household-overhead---the time and effort needed to identify a household, obtain permission to study it, prepare the household for study, etc.---into account.)


# Appendix

## What Parameter Should We Estimate? {#what-parameter}

Someone might argue that, instead of considering the units of study to be the population to be all of the households and estimating the mean of the household per-capita mean daily usage, we should consider the units of study to be household-days, i.e,, each unit is a given day in a given household when its cook-stove is used.  Then the population would be the set of all household-days, and the parameter to be estimated is the mean of the usages over that set.

In this case we would have to consider what sort of sample we have drawn from that population.  Given that we have randomly sampled households and then measured usage for 3-4 days at each household, we are looking at what is known as a "cluster" sample.

Now it is possible to treat a cluster sample as a simple random sample, provided that it is reasonable to believe that the cluster-units---for us: the households in the population---are similar with respect to their per-capita daily usage of wood.

### An Illustrative App

But are the households similar?  One way to investigate this question is to make use of the fact that if the households *were* similar, then each one of the observed per-capita daily measurments could have been observed at any of the houses in the sample---not just at the house where it was actually observed.

in the app below, each time the user presses the action button the observed measurements in the actual study are scrambled randomly among the sample households.  Each household $h_i$, where $1 \leq i \leq 16$, gets the same number $J_i$ of measurements that it had in the actual study, but these $J_i$ measurements are a random sample form the 57 measurments in the study.  The app then computes the average of the measurements for each household and makes a density plot (right column in the main panel) of these 16 averages.  For comparison, a density plot of the 16 averages in the actual study is shown in the left column of the main panel.

Try the app many times, comparing the right and left columns as you go.

**APP GOES HERE**

<!-- Eivory does this app-->

```{r sampling, echo=FALSE}
# shinyAppDir(
#   appDir = "sampling",
#   options = list(
#     width = "100%", height = 550
#   )
# )
```

You will probably notice that, on the whole the averages from scrambled measurements are much less spread apart from each other than are the averages from the actual study.  This is happening because the households are *not* similar with respect to usage:  some households use more wood per-capita on average than do other houses.

### Analysis of Variance

Statisticians might suggest that we quantify the strength of the evidence provided by the data for dis-similarity across households.  One possible way to accomplish this is to compare two models of wood-usage:

* a "null" linear model of usage: a linear model where the response variable is the usage and there are no predictor varaibles;
* a "random-effect" model: a model where the response variable is usage and the predictor variable is the household at which the usage was observed, with the mean usage for each houshold assumed to be normally distributed with mean $\mu$ and some unknown standard deviation).

The comparison is carried out through an *analysis of variance*, which compares how well each model does at the task of "fitting" to the observed data.  When we do the analysis of variance with the R-package **nlme**, we get the following results:

```{r}
mod_null <- lm(wood ~ 1, data = all_meas)
mod_rf <- lme(wood ~ 1, random = ~ 1 | house, data = all_meas)
res <- anova(mod_rf, mod_null)
df <- as.data.frame(res)
knitr::kable(df[, -(1:5)])
```

Among other things, the model computes the natural logarithm of the likelihood of getting the observed data, under each of the model assumptions.  It then states the ratio of these log-likelihoods (log-likelihood for the random-effect model divided by log-likelihood for the null model) .

We see that the ratio is approximately `r round(res$L.Ratio[2], 3)`, quite a large value.  In fact, if the households in the population are in fact uniform with respect to mean daily per-capita usage of wood, then the probabilty of getting that ratio or higher is extremely small:  less than 0.0001 in fact.

We conclude that the data provide extrememly strong evidence that households in the population are dis-similar in their wood-usage.  Thus it is better to choose households as our units of study and to attempt estimate $\mu$, the mean of their mean daily per-capita usages.
